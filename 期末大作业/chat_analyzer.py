"""
å¾®ä¿¡èŠå¤©è®°å½•ç»¼åˆåˆ†æžç³»ç»Ÿ
åŠŸèƒ½ï¼šåŸºç¡€ç»Ÿè®¡ + AIæŠ¥å‘Š + æƒ…æ„Ÿåˆ†æž + æ¨¡æ‹Ÿäº¤äº’ + è¯äº‘ + èšç±»åˆ†æž
"""
import json
import pandas as pd
import numpy as np
import matplotlib
import matplotlib.font_manager as fm
import os
import glob

# ===== å…³é”®ä¿®å¤ï¼šåœ¨å¯¼å…¥ pyplot ä¹‹å‰å…ˆæ¸…ç†ç¼“å­˜å’Œé…ç½®å­—ä½“ =====
print("ðŸ”§ æ­£åœ¨é…ç½®ä¸­æ–‡å­—ä½“æ”¯æŒ...")

# ç¬¬1æ­¥ï¼šæ¸…ç†matplotlibç¼“å­˜ï¼ˆä¸Ž ultimate_test.py ç›¸åŒï¼‰
cache_dir = matplotlib.get_cachedir()
try:
    cache_files = glob.glob(os.path.join(cache_dir, 'fontlist-*.json'))
    for f in cache_files:
        try:
            os.remove(f)
        except:
            pass
except:
    pass

# ç¬¬2æ­¥ï¼šå¼ºåˆ¶é‡æ–°åŠ è½½å­—ä½“ç®¡ç†å™¨
fm._load_fontmanager(try_read_cache=False)

# ç¬¬3æ­¥ï¼šæ·»åŠ ä¸­æ–‡å­—ä½“åˆ°matplotlibå­—ä½“ç®¡ç†å™¨
font_file = 'C:/Windows/Fonts/simhei.ttf'
if os.path.exists(font_file):
    try:
        # ä½¿ç”¨ addfont æ–¹æ³•ï¼ˆmatplotlib 3.2+ï¼‰
        fm.fontManager.addfont(font_file)
        print(f"âœ… å·²æ·»åŠ å­—ä½“æ–‡ä»¶: {font_file}")
    except AttributeError:
        # æ—§ç‰ˆæœ¬matplotlibæ²¡æœ‰addfontæ–¹æ³•
        pass

# ç¬¬4æ­¥ï¼šé…ç½®ä¸­æ–‡å­—ä½“ä¸ºé»˜è®¤å­—ä½“
matplotlib.rcParams['font.sans-serif'] = ['SimHei', 'Microsoft YaHei', 'Arial Unicode MS']
matplotlib.rcParams['font.family'] = 'sans-serif'
matplotlib.rcParams['axes.unicode_minus'] = False
print("âœ… ä¸­æ–‡å­—ä½“é…ç½®å®Œæˆ: SimHei")

# çŽ°åœ¨æ‰å¯¼å…¥ pyplot
import matplotlib.pyplot as plt
import seaborn as sns
from datetime import datetime
from collections import Counter
import requests
import warnings
import jieba
import jieba.analyse
from wordcloud import WordCloud
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.cluster import KMeans
import re

warnings.filterwarnings('ignore')

# å†æ¬¡å¼ºåˆ¶è®¾ç½® pyplot çš„é…ç½®ï¼ˆç¡®ä¿ç”Ÿæ•ˆï¼‰
plt.rcParams['font.sans-serif'] = ['SimHei', 'Microsoft YaHei', 'Arial Unicode MS']
plt.rcParams['font.family'] = 'sans-serif'
plt.rcParams['axes.unicode_minus'] = False

print("âœ… Matplotlib ä¸­æ–‡å­—ä½“è®¾ç½®å®Œæˆ")

# ç®€åŒ–çš„å­—ä½“è®¾ç½®å‡½æ•°ï¼ˆå­—ä½“å·²åœ¨å…¨å±€é…ç½®ï¼‰
def setup_chinese_font():
    """è¿”å›žä¸­æ–‡å­—ä½“å±žæ€§"""
    font_paths = [
        'C:/Windows/Fonts/simhei.ttf',
        'C:/Windows/Fonts/msyh.ttc',
        'C:/Windows/Fonts/simsun.ttc',
        '/System/Library/Fonts/PingFang.ttc',
        '/usr/share/fonts/truetype/wqy/wqy-microhei.ttc'
    ]
    
    for font_path in font_paths:
        if os.path.exists(font_path):
            return fm.FontProperties(fname=font_path)
    return None

# èŽ·å–å­—ä½“å±žæ€§ï¼ˆç”¨äºŽè¯äº‘ç­‰éœ€è¦å­—ä½“è·¯å¾„çš„åœ°æ–¹ï¼‰
font_prop = setup_chinese_font()

# è®¾ç½®seaborné£Žæ ¼ï¼ˆä½†ä¸è¦†ç›–å­—ä½“ï¼‰
sns.set_style("whitegrid")
sns.set_palette("husl")

# ç¡®ä¿ seaborn ä¸è¦†ç›–å­—ä½“è®¾ç½®
plt.rcParams['font.sans-serif'] = ['SimHei', 'Microsoft YaHei', 'Arial Unicode MS']
plt.rcParams['axes.unicode_minus'] = False


class WeChatAnalyzer:
    """å¾®ä¿¡èŠå¤©è®°å½•ç»¼åˆåˆ†æžå™¨"""
    
    def __init__(self, deepseek_api_key=None):
        self.df = None
        self.api_key = deepseek_api_key
        self.stats = {}
        self.sentiment_results = {}
        
    def load_json_data(self, json_file):
        """ä»ŽJSONæ–‡ä»¶åŠ è½½èŠå¤©è®°å½•"""
        print(f"ðŸ“‚ æ­£åœ¨åŠ è½½JSONæ–‡ä»¶: {json_file}")
        
        with open(json_file, 'r', encoding='utf-8') as f:
            data = json.load(f)
        
        # æå–ä¼šè¯ä¿¡æ¯
        session = data.get('session', {})
        messages = data.get('messages', [])
        
        print(f"âœ… ä¼šè¯ä¿¡æ¯:")
        print(f"   å¯¹è±¡: {session.get('displayName', 'Unknown')}")
        print(f"   ç±»åž‹: {session.get('type', 'Unknown')}")
        print(f"   æ¶ˆæ¯æ€»æ•°: {session.get('messageCount', 0)}")
        
        # è§£æžæ¶ˆæ¯
        parsed_messages = []
        for msg in messages:
            # è·³è¿‡æ²¡æœ‰å®Œæ•´ä¿¡æ¯çš„æ¶ˆæ¯
            if 'formattedTime' not in msg or 'content' not in msg:
                continue
            
            # åªå¤„ç†æ–‡æœ¬æ¶ˆæ¯
            if msg.get('type') == 'æ–‡æœ¬æ¶ˆæ¯' or (msg.get('content') and not msg.get('content').startswith('[')):
                parsed_messages.append({
                    'datetime': msg.get('formattedTime'),
                    'sender': msg.get('senderDisplayName', 'Unknown'),
                    'content': msg.get('content', ''),
                    'is_send': msg.get('isSend', 0)
                })
        
        # è½¬æ¢ä¸ºDataFrame
        self.df = pd.DataFrame(parsed_messages)
        
        if len(self.df) == 0:
            print("âš ï¸ æœªæ‰¾åˆ°æœ‰æ•ˆçš„æ–‡æœ¬æ¶ˆæ¯")
            return False
        
        # æ•°æ®é¢„å¤„ç†
        self.df['datetime'] = pd.to_datetime(self.df['datetime'], errors='coerce')
        self.df = self.df.dropna(subset=['datetime', 'content'])
        self.df = self.df[self.df['content'].str.strip() != '']
        
        # æ·»åŠ æ—¶é—´ç›¸å…³å­—æ®µ
        self.df['date'] = self.df['datetime'].dt.date
        self.df['hour'] = self.df['datetime'].dt.hour
        self.df['weekday'] = self.df['datetime'].dt.weekday
        self.df['message_length'] = self.df['content'].astype(str).str.len()
        
        print(f"âœ… æˆåŠŸåŠ è½½ {len(self.df)} æ¡æœ‰æ•ˆæ¶ˆæ¯")
        print(f"ðŸ“… æ—¶é—´èŒƒå›´: {self.df['date'].min()} è‡³ {self.df['date'].max()}")
        print(f"ðŸ‘¥ å‚ä¸Žè€…: {', '.join(self.df['sender'].unique())}")
        
        return True
    
    def basic_analysis(self):
        """åŸºç¡€ç»Ÿè®¡åˆ†æž"""
        print("\n" + "="*60)
        print("ðŸ“Š åŸºç¡€ç»Ÿè®¡åˆ†æž")
        print("="*60)
        
        stats = {}
        
        # æ€»ä½“ç»Ÿè®¡
        stats['total_messages'] = len(self.df)
        stats['date_range'] = f"{self.df['date'].min()} è‡³ {self.df['date'].max()}"
        stats['duration_days'] = (self.df['date'].max() - self.df['date'].min()).days
        
        print(f"\nðŸ“ˆ æ€»ä½“æƒ…å†µ:")
        print(f"   æ¶ˆæ¯æ€»æ•°: {stats['total_messages']} æ¡")
        print(f"   æ—¶é—´è·¨åº¦: {stats['duration_days']} å¤©")
        print(f"   æ—¥å‡æ¶ˆæ¯: {stats['total_messages'] / max(stats['duration_days'], 1):.1f} æ¡")
        
        # å‘é€è€…ç»Ÿè®¡
        sender_counts = self.df['sender'].value_counts()
        print(f"\nðŸ‘¥ å‚ä¸Žè€…åˆ†å¸ƒ:")
        for sender, count in sender_counts.items():
            percentage = count / len(self.df) * 100
            print(f"   {sender}: {count} æ¡ ({percentage:.1f}%)")
            stats[f'{sender}_count'] = count
            stats[f'{sender}_percentage'] = percentage
        
        # æ¶ˆæ¯é•¿åº¦ç»Ÿè®¡
        print(f"\nðŸ’¬ æ¶ˆæ¯é•¿åº¦:")
        print(f"   å¹³å‡é•¿åº¦: {self.df['message_length'].mean():.1f} å­—ç¬¦")
        print(f"   æœ€é•¿æ¶ˆæ¯: {self.df['message_length'].max()} å­—ç¬¦")
        print(f"   æœ€çŸ­æ¶ˆæ¯: {self.df['message_length'].min()} å­—ç¬¦")
        
        # æ´»è·ƒæ—¶æ®µ
        hourly = self.df['hour'].value_counts().sort_index()
        peak_hour = hourly.idxmax()
        print(f"\nâ° æ´»è·ƒæ—¶æ®µ:")
        print(f"   æœ€æ´»è·ƒ: {peak_hour}:00 ({hourly[peak_hour]} æ¡)")
        
        # é«˜é¢‘è¯æ±‡
        all_text = ' '.join(self.df['content'].astype(str))
        words = [w for w in all_text if len(w) > 1]
        word_freq = Counter(words).most_common(10)
        print(f"\nðŸ”¤ é«˜é¢‘å­—ç¬¦:")
        for word, freq in word_freq[:5]:
            print(f"   '{word}': {freq} æ¬¡")
        
        self.stats = stats
        return stats
    
    def simple_sentiment_analysis(self):
        """ç®€å•æƒ…æ„Ÿåˆ†æžï¼ˆåŸºäºŽå…³é”®è¯ï¼‰"""
        print("\n" + "="*60)
        print("ðŸ˜Š æƒ…æ„Ÿå€¾å‘åˆ†æž")
        print("="*60)
        
        # å®šä¹‰æƒ…æ„Ÿå…³é”®è¯
        positive_words = ['å“ˆå“ˆ', 'ðŸ˜Š', 'ðŸ˜„', 'ðŸ‘', 'å¥½çš„', 'è°¢è°¢', 'å–œæ¬¢', 'å¼€å¿ƒ', 'æ£’', 'çˆ±', 
                         'å˜¿å˜¿', 'å—¯å—¯', 'å¯ä»¥', 'ä¸é”™', 'åŽ‰å®³', 'èµž', 'å“‡', 'å¤ªå¥½äº†', 'ðŸ˜', 'â¤ï¸']
        negative_words = ['ðŸ˜¢', 'ðŸ˜­', 'éš¾è¿‡', 'ä¸å¥½', 'è®¨åŽŒ', 'çƒ¦', 'æ°”', 'ç´¯', 'å”‰', 'ç³Ÿç³•',
                         'ä¸è¡Œ', 'ç®—äº†', 'æ— èŠ', 'çƒ¦äºº', 'ðŸ˜¤', 'ðŸ’”']
        
        sentiment_labels = []
        
        for content in self.df['content']:
            content_str = str(content)
            has_positive = any(word in content_str for word in positive_words)
            has_negative = any(word in content_str for word in negative_words)
            
            if has_positive and not has_negative:
                sentiment_labels.append('ç§¯æž')
            elif has_negative and not has_positive:
                sentiment_labels.append('æ¶ˆæž')
            else:
                sentiment_labels.append('ä¸­æ€§')
        
        # æ·»åŠ åˆ°DataFrame
        self.df['sentiment'] = sentiment_labels
        
        # ç»Ÿè®¡
        sentiment_counts = self.df['sentiment'].value_counts()
        positive_count = sentiment_counts.get('ç§¯æž', 0)
        neutral_count = sentiment_counts.get('ä¸­æ€§', 0)
        negative_count = sentiment_counts.get('æ¶ˆæž', 0)
        
        total = len(self.df)
        print(f"\næƒ…æ„Ÿåˆ†å¸ƒ:")
        print(f"   ðŸ˜Š ç§¯æž: {positive_count} æ¡ ({positive_count/total*100:.1f}%)")
        print(f"   ðŸ˜ ä¸­æ€§: {neutral_count} æ¡ ({neutral_count/total*100:.1f}%)")
        print(f"   ðŸ˜¢ æ¶ˆæž: {negative_count} æ¡ ({negative_count/total*100:.1f}%)")
        
        self.sentiment_results = {
            'positive': positive_count,
            'neutral': neutral_count,
            'negative': negative_count
        }
        
        return self.sentiment_results
    
    def word_frequency_analysis(self):
        """è¯é¢‘ç»Ÿè®¡åˆ†æž"""
        print("\n" + "="*60)
        print("ðŸ“ è¯é¢‘ç»Ÿè®¡åˆ†æž")
        print("="*60)
        
        # åˆå¹¶æ‰€æœ‰æ¶ˆæ¯
        all_text = ' '.join(self.df['content'].astype(str))
        
        # åˆ†è¯
        print("ðŸ”„ æ­£åœ¨åˆ†è¯...")
        words = jieba.cut(all_text)
        
        # æ‰©å±•åœç”¨è¯åˆ—è¡¨ - è¿‡æ»¤æ— ç”¨è¯æ±‡
        stopwords = {
            # åŸºç¡€åœç”¨è¯
            'çš„', 'äº†', 'æ˜¯', 'æˆ‘', 'ä½ ', 'ä»–', 'å¥¹', 'å®ƒ', 'åœ¨', 'æœ‰', 'å’Œ', 'å°±', 'ä¸', 
            'äºº', 'éƒ½', 'ä¸€', 'ä¸€ä¸ª', 'ä¸Š', 'ä¹Ÿ', 'å¾ˆ', 'åˆ°', 'è¯´', 'è¦', 'åŽ»', 'å—',
            'ä¼š', 'èƒ½', 'æ²¡', 'çœ‹', 'å¥½', 'è‡ªå·±', 'è¿™', 'é‚£', 'å•Š', 'å‘€', 'å“¦', 'å—¯',
            # ç¼€è¯å’Œè¯­æ°”è¯
            'è¿™ä¸ª', 'é‚£ä¸ª', 'å“ˆå“ˆ', 'å“ˆå“ˆå“ˆ', 'å“ˆå“ˆå“ˆå“ˆ', 'å“ˆå“ˆå“ˆå“ˆå“ˆ', 'å˜¿å˜¿', 'å˜»å˜»',
            'å‘µå‘µ', 'å—¯å—¯', 'å—¯å‘¢', 'å“¦å“¦', 'å•¦å•¦', 'å‘€å‘€', 'å§å§', 'å‘¢å‘¢',
            'å°±æ˜¯', 'ç„¶åŽ', 'ä½†æ˜¯', 'è¿˜æ˜¯', 'å¯ä»¥', 'çŽ°åœ¨', 'è§‰å¾—', 'æ„Ÿè§‰', 'çœŸçš„', 'æˆ‘ä»¬',
            'ä»€ä¹ˆ', 'æ€Žä¹ˆ', 'ä¸ºä»€ä¹ˆ', 'è¿™æ ·', 'é‚£æ ·', 'è¿™ä¹ˆ', 'é‚£ä¹ˆ', 'å¤šå°‘', 'å“ªé‡Œ',
            # æ–‡ä»¶æ ¼å¼å’Œè·¯å¾„ç›¸å…³
            'media', 'emojis', 'gif', 'jpg', 'png', 'jpeg', 'mp4', 'mp3', 'pdf', 'doc',
            'images', 'voices', 'videos', 'files', 'http', 'https', 'www', 'com', 'cn',
            # æ ‡ç‚¹ç¬¦å·
            '[å›¾ç‰‡]', '[è¡¨æƒ…]', '[è¯­éŸ³]', '[è§†é¢‘]', '[æ–‡ä»¶]', '[é“¾æŽ¥]',
            'ï¼Œ', 'ã€‚', 'ï¼', 'ï¼Ÿ', 'ï¼›', 'ï¼š', '"', '"', ''', ''', 'ã€', 'â€¦', 'â€”',
            # ç©ºç™½å­—ç¬¦
            ' ', '\n', '\t', '\r',
            # å•å­—
            'ä¸ª', 'äº›', 'ç§', 'æ ·', 'å—', 'æŠŠ', 'å¼ ', 'åª', 'æ¬¡', 'ä¸‹', 'å¤©', 'å¹´',
            # æ— æ„ä¹‰è¯æ±‡
            'éžå¸¸', 'ç‰¹åˆ«', 'æ¯”è¾ƒ', 'ç¨å¾®', 'æœ‰ç‚¹', 'ä¸€ç‚¹', 'ä¸€äº›', 'ä¸€ä¸‹'
        }
        
        # æ–‡ä»¶æ ¼å¼åŒ¹é…æ¨¡å¼
        file_patterns = ['media', 'emojis', 'gif', 'jpg', 'png', 'jpeg', 'mp4', 'voices', 'images']
        
        # è¿‡æ»¤è¯æ±‡
        filtered_words = []
        for w in words:
            # è¿‡æ»¤æ¡ä»¶:
            # 1. é•¿åº¦å¤§äºŽ1
            # 2. ä¸åœ¨åœç”¨è¯è¡¨ä¸­
            # 3. ä¸åŒ…å«æ–‡ä»¶æ ¼å¼ç›¸å…³å­—ç¬¦
            # 4. ä¸æ˜¯çº¯æ•°å­—
            # 5. ä¸æ˜¯çº¯è‹±æ–‡(é™¤éžæ˜¯æœ‰æ„ä¹‰çš„é•¿å•è¯)
            if (len(w) > 1 and 
                w not in stopwords and 
                not any(pattern in w.lower() for pattern in file_patterns) and
                not w.isdigit() and
                not (w.encode('UTF-8').isalpha() and len(w) < 4)):  # è¿‡æ»¤çŸ­è‹±æ–‡
                filtered_words.append(w)
        
        # ç»Ÿè®¡è¯é¢‘
        word_freq = Counter(filtered_words)
        top_words = word_freq.most_common(30)
        
        print(f"\nðŸ”¤ é«˜é¢‘è¯æ±‡ TOP 20:")
        for i, (word, freq) in enumerate(top_words[:20], 1):
            print(f"   {i:2d}. {word:8s} - {freq:4d} æ¬¡")
        
        self.word_freq = word_freq
        return word_freq
    
    def topic_clustering(self):
        """ä¸»é¢˜èšç±»åˆ†æž"""
        print("\n" + "="*60)
        print("ðŸŽ¯ èŠå¤©ä¸»é¢˜èšç±»åˆ†æž")
        print("="*60)
        
        # å‡†å¤‡æ–‡æœ¬æ•°æ®
        texts = self.df['content'].astype(str).tolist()
        
        # åˆ†è¯å¤„ç†
        print("ðŸ”„ æ­£åœ¨å¤„ç†æ–‡æœ¬...")
        processed_texts = []
        
        # æ‰©å±•åœç”¨è¯åˆ—è¡¨ - ä¸Žè¯é¢‘åˆ†æžä¿æŒä¸€è‡´
        stopwords = {
            # åŸºç¡€åœç”¨è¯
            'çš„', 'äº†', 'æ˜¯', 'æˆ‘', 'ä½ ', 'ä»–', 'å¥¹', 'å®ƒ', 'åœ¨', 'æœ‰', 'å’Œ', 'å°±', 'ä¸',
            'å•Š', 'å‘€', 'å“¦', 'å—¯', 'å—', 'å‘¢',
            # ç¼€è¯å’Œè¯­æ°”è¯
            'è¿™ä¸ª', 'é‚£ä¸ª', 'å“ˆå“ˆ', 'å“ˆå“ˆå“ˆ', 'å“ˆå“ˆå“ˆå“ˆ', 'å˜¿å˜¿', 'å˜»å˜»', 'å‘µå‘µ',
            'å°±æ˜¯', 'ç„¶åŽ', 'ä½†æ˜¯', 'è¿˜æ˜¯', 'å¯ä»¥', 'çŽ°åœ¨', 'è§‰å¾—', 'æ„Ÿè§‰', 'çœŸçš„', 'æˆ‘ä»¬',
            'ä»€ä¹ˆ', 'æ€Žä¹ˆ', 'è¿™æ ·', 'é‚£æ ·',
            # æ–‡ä»¶æ ¼å¼ç›¸å…³
            'media', 'emojis', 'gif', 'jpg', 'png', 'jpeg', 'mp4', 'voices', 'images'
        }
        
        file_patterns = ['media', 'emojis', 'gif', 'jpg', 'png', 'jpeg', 'mp4', 'voices', 'images']
        
        for text in texts:
            words = jieba.cut(text)
            # è¿‡æ»¤åœç”¨è¯å’Œæ— ç”¨è¯æ±‡
            filtered = []
            for w in words:
                if (len(w) > 1 and 
                    w not in stopwords and 
                    not any(pattern in w.lower() for pattern in file_patterns) and
                    not w.isdigit()):
                    filtered.append(w)
            processed_texts.append(' '.join(filtered))
        
        # TF-IDFå‘é‡åŒ–
        try:
            vectorizer = TfidfVectorizer(max_features=100, min_df=2)
            X = vectorizer.fit_transform(processed_texts)
            
            # K-meansèšç±»
            n_clusters = min(5, len(self.df) // 100 + 1)  # æ ¹æ®æ•°æ®é‡åŠ¨æ€è°ƒæ•´èšç±»æ•°
            kmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init=10)
            clusters = kmeans.fit_predict(X)
            
            # æ·»åŠ èšç±»æ ‡ç­¾åˆ°DataFrame
            self.df['cluster'] = clusters
            
            # åˆ†æžæ¯ä¸ªèšç±»çš„å…³é”®è¯
            print(f"\nðŸ“Š è¯†åˆ«å‡º {n_clusters} ä¸ªä¸»é¢˜èšç±»:\n")
            
            feature_names = vectorizer.get_feature_names_out()
            cluster_topics = {}
            
            for i in range(n_clusters):
                cluster_center = kmeans.cluster_centers_[i]
                top_indices = cluster_center.argsort()[-15:][::-1]  # æå–æ›´å¤šå…³é”®è¯
                top_words = [feature_names[idx] for idx in top_indices]
                
                cluster_size = (clusters == i).sum()
                cluster_percentage = cluster_size / len(clusters) * 100
                
                # æŽ¨æµ‹ä¸»é¢˜
                topic_name = self._guess_topic_name(top_words)
                
                # æå–è¯¥èšç±»çš„ç¤ºä¾‹æ¶ˆæ¯ - ä¼˜åŒ–é€‰æ‹©ç­–ç•¥
                cluster_df = self.df[self.df['cluster'] == i].copy()
                
                # è¿‡æ»¤æŽ‰è¿‡çŸ­æˆ–åŒ…å«æ–‡ä»¶è·¯å¾„çš„æ¶ˆæ¯
                valid_messages = cluster_df[
                    (cluster_df['content'].str.len() >= 10) &  # è‡³å°‘10ä¸ªå­—ç¬¦
                    (~cluster_df['content'].str.contains('media|emojis|gif|jpg|png', case=False, na=False))
                ]['content']
                
                # å¦‚æžœæœ‰æœ‰æ•ˆæ¶ˆæ¯ï¼Œéšæœºé€‰æ‹©3æ¡ï¼›å¦åˆ™ä½¿ç”¨åŽŸå§‹æ¶ˆæ¯
                if len(valid_messages) >= 3:
                    cluster_messages = valid_messages.sample(min(3, len(valid_messages))).tolist()
                elif len(valid_messages) > 0:
                    cluster_messages = valid_messages.tolist()
                else:
                    # å¦‚æžœæ²¡æœ‰æœ‰æ•ˆæ¶ˆæ¯ï¼Œé€‰æ‹©æœ€é•¿çš„3æ¡
                    cluster_messages = cluster_df.nlargest(3, 'message_length')['content'].tolist()
                
                print(f"   â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”")
                print(f"   ä¸»é¢˜ {i+1}: {topic_name}")
                print(f"   æ¶ˆæ¯æ•°: {cluster_size} æ¡ ({cluster_percentage:.1f}%)")
                print(f"   æ ¸å¿ƒå…³é”®è¯: {', '.join(top_words[:10])}")
                print(f"   å…¸åž‹æ¶ˆæ¯ç¤ºä¾‹:")
                for j, msg in enumerate(cluster_messages[:3], 1):
                    # æˆªæ–­è¿‡é•¿çš„æ¶ˆæ¯å¹¶æ¸…ç†
                    display_msg = str(msg).replace('\n', ' ').strip()
                    display_msg = display_msg[:60] + '...' if len(display_msg) > 60 else display_msg
                    print(f"      {j}. {display_msg}")
                print()
                
                cluster_topics[i] = {
                    'name': topic_name,
                    'size': cluster_size,
                    'keywords': top_words[:15],
                    'examples': cluster_messages[:3]
                }
            
            self.cluster_topics = cluster_topics
            return cluster_topics
            
        except Exception as e:
            print(f"âš ï¸ èšç±»åˆ†æžå¤±è´¥: {e}")
            print("   (å¯èƒ½æ˜¯æ•°æ®é‡å¤ªå°‘æˆ–æ–‡æœ¬ç›¸ä¼¼åº¦è¿‡é«˜)")
            return None
    
    def _guess_topic_name(self, keywords):
        """æ ¹æ®å…³é”®è¯æŽ¨æµ‹ä¸»é¢˜åç§° - æ›´å…·ä½“åŒ–"""
        keywords_str = ' '.join(keywords)
        
        # å®šä¹‰æ›´å…·ä½“çš„ä¸»é¢˜è§„åˆ™ï¼ŒæŒ‰ä¼˜å…ˆçº§åŒ¹é…
        # å­¦ä¹ ç›¸å…³
        if any(w in keywords_str for w in ['ä½œä¸š', 'è€ƒè¯•', 'è¯•å·', 'æˆç»©', 'åˆ†æ•°']):
            return "ðŸ“ ä½œä¸šè€ƒè¯•"
        elif any(w in keywords_str for w in ['è¯¾ç¨‹', 'ä¸Šè¯¾', 'è€å¸ˆ', 'æ•™æŽˆ', 'è®²è¯¾']):
            return "ðŸ“š è¯¾ç¨‹å­¦ä¹ "
        elif any(w in keywords_str for w in ['è®ºæ–‡', 'ç ”ç©¶', 'å®žéªŒ', 'é¡¹ç›®', 'paper']):
            return "ðŸ”¬ å­¦æœ¯ç ”ç©¶"
        elif any(w in keywords_str for w in ['å­¦ä¹ ', 'å¤ä¹ ', 'é¢„ä¹ ', 'èƒŒä¹¦', 'çœ‹ä¹¦']):
            return "ðŸ“– è‡ªä¸»å­¦ä¹ "
        
        # ç”Ÿæ´»ç›¸å…³
        elif any(w in keywords_str for w in ['æ—©é¥­', 'åˆé¥­', 'æ™šé¥­', 'åƒé¥­', 'é£Ÿå ‚', 'å¤–å–']):
            return "ðŸ” ç”¨é¤è¯é¢˜"
        elif any(w in keywords_str for w in ['å¥½åƒ', 'ç¾Žé£Ÿ', 'é¤åŽ…', 'èœ', 'å‘³é“']):
            return "ðŸ˜‹ ç¾Žé£Ÿåˆ†äº«"
        elif any(w in keywords_str for w in ['ç¡è§‰', 'èµ·åºŠ', 'å›°', 'ç´¯', 'ä¼‘æ¯']):
            return "ðŸ˜´ ä½œæ¯æ—¶é—´"
        elif any(w in keywords_str for w in ['å®¿èˆ', 'å¯å®¤', 'å®¤å‹', 'èˆå‹']):
            return "ðŸ  å®¿èˆç”Ÿæ´»"
        
        # å¨±ä¹ç›¸å…³
        elif any(w in keywords_str for w in ['æ¸¸æˆ', 'æ‰“æ¸¸æˆ', 'çŽ©æ¸¸æˆ', 'å¼€é»‘', 'ä¸Šåˆ†']):
            return "ðŸŽ® æ¸¸æˆå¨±ä¹"
        elif any(w in keywords_str for w in ['ç”µå½±', 'ç”µè§†å‰§', 'ç»¼è‰º', 'è¿½å‰§']):
            return "ðŸŽ¬ å½±è§†å‰§é›†"
        elif any(w in keywords_str for w in ['éŸ³ä¹', 'æ­Œ', 'å”±æ­Œ', 'å¬æ­Œ']):
            return "ï¿½ éŸ³ä¹è¯é¢˜"
        elif any(w in keywords_str for w in ['è¿åŠ¨', 'è·‘æ­¥', 'å¥èº«', 'æ‰“çƒ', 'é”»ç‚¼']):
            return "âš½ è¿åŠ¨å¥èº«"
        
        # ç¤¾äº¤ç›¸å…³
        elif any(w in keywords_str for w in ['èšä¼š', 'æ´»åŠ¨', 'èšé¤', 'å‡ºåŽ»çŽ©']):
            return "ðŸŽ‰ èšä¼šæ´»åŠ¨"
        elif any(w in keywords_str for w in ['æœ‹å‹', 'åŒå­¦', 'è®¤è¯†', 'ä»‹ç»']):
            return "ðŸ‘¥ ç¤¾äº¤äº’åŠ¨"
        elif any(w in keywords_str for w in ['è´­ç‰©', 'ä¹°', 'æ·˜å®', 'å•†å“', 'ä»·æ ¼']):
            return "ðŸ›’ è´­ç‰©æ¶ˆè´¹"
        
        # å·¥ä½œç›¸å…³
        elif any(w in keywords_str for w in ['å·¥ä½œ', 'å®žä¹ ', 'å…¬å¸', 'é¢è¯•', 'æ±‚èŒ']):
            return "ðŸ’¼ å·¥ä½œå®žä¹ "
        elif any(w in keywords_str for w in ['ä¼šè®®', 'å¼€ä¼š', 'æ±‡æŠ¥', 'é¢†å¯¼']):
            return "ï¿½ ä¼šè®®å·¥ä½œ"
        
        # æƒ…æ„Ÿç›¸å…³
        elif any(w in keywords_str for w in ['å¼€å¿ƒ', 'é«˜å…´', 'å¿«ä¹', 'å–œæ¬¢', 'çˆ±']):
            return "ï¿½ å¼€å¿ƒåˆ†äº«"
        elif any(w in keywords_str for w in ['éš¾è¿‡', 'ä¼¤å¿ƒ', 'éš¾å—', 'éƒé—·']):
            return "ðŸ˜¢ å€¾è¯‰çƒ¦æ¼"
        elif any(w in keywords_str for w in ['ç”Ÿæ°”', 'æ°”', 'çƒ¦', 'è®¨åŽŒ']):
            return "ï¿½ æƒ…ç»ªå‘æ³„"
        
        # å…¶ä»–
        elif any(w in keywords_str for w in ['å¤©æ°”', 'ä¸‹é›¨', 'æ™´å¤©', 'å†·', 'çƒ­']):
            return "ðŸŒ¤ï¸ å¤©æ°”è¯é¢˜"
        elif any(w in keywords_str for w in ['æ—¶é—´', 'åœ°ç‚¹', 'ä»€ä¹ˆæ—¶å€™', 'å“ªé‡Œ']):
            return "ï¿½ æ—¶é—´åœ°ç‚¹"
        else:
            return "ðŸ’¬ æ—¥å¸¸é—²èŠ"
    
    def create_visualizations(self):
        """ç”Ÿæˆç²¾ç¾Žçš„å¯è§†åŒ–å›¾è¡¨"""
        print("\n" + "="*60)
        print("ðŸ“Š ç”Ÿæˆå¯è§†åŒ–å›¾è¡¨")
        print("="*60)
        
        # åˆ›å»ºè¾“å‡ºç›®å½•
        os.makedirs('analysis_results', exist_ok=True)
        
        # ===== å…³é”®ï¼šä¸ä½¿ç”¨ style.useï¼Œé¿å…é‡ç½®å­—ä½“è®¾ç½® =====
        # ç›´æŽ¥è®¾ç½®æ‰€éœ€çš„æ ·å¼å‚æ•°
        plt.rcParams.update({
            'font.sans-serif': ['SimHei', 'Microsoft YaHei', 'Arial Unicode MS'],
            'font.family': 'sans-serif',
            'axes.unicode_minus': False,
            'axes.grid': True,
            'grid.alpha': 0.3,
            'grid.linestyle': '--',
            'figure.facecolor': 'white',
            'axes.facecolor': '#f0f0f0'
        })
        
        colors = ['#FF6B6B', '#4ECDC4', '#45B7D1', '#FFA07A', '#98D8C8', '#F7DC6F']
        
        # 1. æ¯æ—¥æ¶ˆæ¯è¶‹åŠ¿å›¾ï¼ˆæ›´ç¾Žè§‚ï¼‰
        print("ðŸ“ˆ ç”Ÿæˆæ¯æ—¥è¶‹åŠ¿å›¾...")
        fig1 = plt.figure(figsize=(14, 7))
        ax1 = plt.subplot(111)
        
        daily = self.df.groupby('date').size()
        
        # ç»˜åˆ¶é¢ç§¯å›¾
        ax1.fill_between(daily.index, daily.values, alpha=0.3, color='#4ECDC4')
        ax1.plot(daily.index, daily.values, marker='o', linewidth=2.5, 
                color='#2E86AB', markersize=5, markerfacecolor='#FF6B6B')
        
        # æ·»åŠ è¶‹åŠ¿çº¿
        z = np.polyfit(range(len(daily)), daily.values, 2)
        p = np.poly1d(z)
        ax1.plot(daily.index, p(range(len(daily))), "--", 
                linewidth=2, alpha=0.5, color='#E74C3C', label='è¶‹åŠ¿çº¿')
        
        ax1.set_title('ðŸ“ˆ æ¯æ—¥æ¶ˆæ¯æ•°é‡è¶‹åŠ¿åˆ†æž', fontsize=18, fontweight='bold', pad=20)
        ax1.set_xlabel('æ—¥æœŸ', fontsize=13, fontweight='bold')
        ax1.set_ylabel('æ¶ˆæ¯æ•°é‡', fontsize=13, fontweight='bold')
        ax1.legend(loc='best', fontsize=11)
        ax1.grid(True, alpha=0.3, linestyle='--')
        
        # ç¾ŽåŒ–åˆ»åº¦
        plt.xticks(rotation=45, ha='right')
        ax1.spines['top'].set_visible(False)
        ax1.spines['right'].set_visible(False)
        
        plt.tight_layout()
        plt.savefig('analysis_results/01_daily_trend.png', dpi=200, bbox_inches='tight')
        print("   âœ… ä¿å­˜: 01_daily_trend.png")
        plt.close()
        
        # 2. å‘é€è€…åˆ†å¸ƒï¼ˆåŒå›¾å±•ç¤ºï¼‰
        print("ðŸ‘¥ ç”Ÿæˆå‘é€è€…åˆ†å¸ƒå›¾...")
        fig2 = plt.figure(figsize=(14, 6))
        
        sender_counts = self.df['sender'].value_counts()
        
        # å·¦å›¾ï¼šé¥¼å›¾
        ax2_1 = plt.subplot(121)
        explode = [0.05] * len(sender_counts)
        wedges, texts, autotexts = ax2_1.pie(
            sender_counts.values, 
            labels=sender_counts.index,
            autopct='%1.1f%%',
            startangle=90,
            colors=colors[:len(sender_counts)],
            explode=explode,
            shadow=True,
            textprops={'fontsize': 11, 'weight': 'bold'}
        )
        for autotext in autotexts:
            autotext.set_color('white')
            autotext.set_fontsize(12)
        ax2_1.set_title('æ¶ˆæ¯å‘é€è€…å æ¯”', fontsize=14, fontweight='bold', pad=15)
        
        # å³å›¾ï¼šæŸ±çŠ¶å›¾
        ax2_2 = plt.subplot(122)
        bars = ax2_2.barh(sender_counts.index, sender_counts.values, 
                          color=colors[:len(sender_counts)], alpha=0.8)
        
        # æ·»åŠ æ•°å€¼æ ‡ç­¾
        for i, (bar, value) in enumerate(zip(bars, sender_counts.values)):
            ax2_2.text(value, i, f' {value}æ¡', va='center', fontsize=11, fontweight='bold')
        
        ax2_2.set_title('æ¶ˆæ¯æ•°é‡å¯¹æ¯”', fontsize=14, fontweight='bold', pad=15)
        ax2_2.set_xlabel('æ¶ˆæ¯æ•°é‡', fontsize=12, fontweight='bold')
        ax2_2.spines['top'].set_visible(False)
        ax2_2.spines['right'].set_visible(False)
        ax2_2.grid(axis='x', alpha=0.3, linestyle='--')
        
        plt.tight_layout()
        plt.savefig('analysis_results/02_sender_distribution.png', dpi=200, bbox_inches='tight')
        print("   âœ… ä¿å­˜: 02_sender_distribution.png")
        plt.close()
        
        # 3. 24å°æ—¶æ´»è·ƒåº¦çƒ­åŠ›å›¾
        print("â° ç”Ÿæˆæ´»è·ƒåº¦åˆ†æžå›¾...")
        fig3 = plt.figure(figsize=(14, 6))
        
        hourly = self.df['hour'].value_counts().sort_index()
        
        # åˆ›å»ºæ¸å˜è‰²æŸ±çŠ¶å›¾
        ax3 = plt.subplot(111)
        bars = ax3.bar(hourly.index, hourly.values, 
                      color=plt.cm.YlOrRd(hourly.values / hourly.values.max()),
                      edgecolor='navy', linewidth=1.5, alpha=0.85)
        
        # é«˜äº®æœ€æ´»è·ƒæ—¶æ®µ
        max_hour = hourly.idxmax()
        bars[max_hour].set_color('#E74C3C')
        bars[max_hour].set_linewidth(3)
        bars[max_hour].set_edgecolor('darkred')
        
        # æ·»åŠ æ•°å€¼æ ‡ç­¾
        for i, (hour, value) in enumerate(hourly.items()):
            if value > hourly.mean():
                ax3.text(hour, value, str(value), ha='center', va='bottom',
                        fontsize=9, fontweight='bold')
        
        ax3.set_title(f'â° 24å°æ—¶æ´»è·ƒåº¦åˆ†å¸ƒ (å³°å€¼: {max_hour}:00)', 
                     fontsize=16, fontweight='bold', pad=20)
        ax3.set_xlabel('å°æ—¶', fontsize=13, fontweight='bold')
        ax3.set_ylabel('æ¶ˆæ¯æ•°é‡', fontsize=13, fontweight='bold')
        ax3.set_xticks(range(24))
        ax3.grid(axis='y', alpha=0.3, linestyle='--')
        ax3.spines['top'].set_visible(False)
        ax3.spines['right'].set_visible(False)
        
        # æ·»åŠ æ—¶æ®µèƒŒæ™¯
        ax3.axvspan(0, 6, alpha=0.1, color='blue', label='å‡Œæ™¨')
        ax3.axvspan(6, 12, alpha=0.1, color='yellow', label='ä¸Šåˆ')
        ax3.axvspan(12, 18, alpha=0.1, color='orange', label='ä¸‹åˆ')
        ax3.axvspan(18, 24, alpha=0.1, color='purple', label='æ™šä¸Š')
        ax3.legend(loc='upper left', fontsize=10)
        
        plt.tight_layout()
        plt.savefig('analysis_results/03_hourly_activity.png', dpi=200, bbox_inches='tight')
        print("   âœ… ä¿å­˜: 03_hourly_activity.png")
        plt.close()
        
        # 4. æƒ…æ„Ÿåˆ†å¸ƒå¯è§†åŒ–
        print("ðŸ˜Š ç”Ÿæˆæƒ…æ„Ÿåˆ†æžå›¾...")
        fig4 = plt.figure(figsize=(14, 6))
        
        sentiment_counts = self.df['sentiment'].value_counts()
        
        # å·¦å›¾ï¼šç”œç”œåœˆå›¾
        ax4_1 = plt.subplot(121)
        sentiment_colors = {'ç§¯æž': '#2ECC71', 'ä¸­æ€§': '#95A5A6', 'æ¶ˆæž': '#E74C3C'}
        colors_list = [sentiment_colors.get(s, '#95A5A6') for s in sentiment_counts.index]
        
        wedges, texts, autotexts = ax4_1.pie(
            sentiment_counts.values,
            labels=sentiment_counts.index,
            autopct='%1.1f%%',
            startangle=90,
            colors=colors_list,
            wedgeprops=dict(width=0.5, edgecolor='white'),
            textprops={'fontsize': 12, 'weight': 'bold'}
        )
        
        for autotext in autotexts:
            autotext.set_color('white')
            autotext.set_fontsize(13)
        
        ax4_1.set_title('ðŸ˜Š æƒ…æ„Ÿå€¾å‘åˆ†å¸ƒ', fontsize=14, fontweight='bold', pad=15)
        
        # å³å›¾ï¼šæŒ‰æ—¶é—´çš„æƒ…æ„Ÿè¶‹åŠ¿
        ax4_2 = plt.subplot(122)
        
        # æŒ‰æ—¥æœŸç»Ÿè®¡æƒ…æ„Ÿ
        sentiment_by_date = self.df.groupby(['date', 'sentiment']).size().unstack(fill_value=0)
        
        if len(sentiment_by_date) > 1:
            for sentiment in ['ç§¯æž', 'ä¸­æ€§', 'æ¶ˆæž']:
                if sentiment in sentiment_by_date.columns:
                    ax4_2.plot(sentiment_by_date.index, sentiment_by_date[sentiment],
                             marker='o', label=sentiment, linewidth=2,
                             color=sentiment_colors.get(sentiment, '#95A5A6'),
                             alpha=0.7)
            
            ax4_2.set_title('æƒ…æ„Ÿè¶‹åŠ¿å˜åŒ–', fontsize=14, fontweight='bold', pad=15)
            ax4_2.set_xlabel('æ—¥æœŸ', fontsize=12)
            ax4_2.set_ylabel('æ¶ˆæ¯æ•°é‡', fontsize=12)
            ax4_2.legend(loc='best', fontsize=11)
            ax4_2.grid(True, alpha=0.3, linestyle='--')
            plt.xticks(rotation=45)
            ax4_2.spines['top'].set_visible(False)
            ax4_2.spines['right'].set_visible(False)
        
        plt.tight_layout()
        plt.savefig('analysis_results/04_sentiment_analysis.png', dpi=200, bbox_inches='tight')
        print("   âœ… ä¿å­˜: 04_sentiment_analysis.png")
        plt.close()
        
        # 5. è¯äº‘å›¾
        if hasattr(self, 'word_freq') and self.word_freq:
            print("â˜ï¸ ç”Ÿæˆè¯äº‘å›¾...")
            fig5 = plt.figure(figsize=(14, 8))
            
            # ç”Ÿæˆè¯äº‘
            try:
                # å°è¯•ä½¿ç”¨ä¸­æ–‡å­—ä½“
                font_paths = [
                    'C:/Windows/Fonts/simhei.ttf',
                    'C:/Windows/Fonts/msyh.ttc',
                    '/System/Library/Fonts/PingFang.ttc'
                ]
                font_path = None
                for fp in font_paths:
                    if os.path.exists(fp):
                        font_path = fp
                        break
                
                wordcloud = WordCloud(
                    width=1400,
                    height=800,
                    background_color='white',
                    font_path=font_path,
                    colormap='viridis',
                    max_words=100,
                    relative_scaling=0.5,
                    min_font_size=10
                ).generate_from_frequencies(self.word_freq)
                
                plt.imshow(wordcloud, interpolation='bilinear')
                plt.axis('off')
                plt.title('â˜ï¸ é«˜é¢‘è¯æ±‡äº‘å›¾', fontsize=20, fontweight='bold', pad=20)
                
                plt.tight_layout()
                plt.savefig('analysis_results/05_wordcloud.png', dpi=200, bbox_inches='tight')
                print("   âœ… ä¿å­˜: 05_wordcloud.png")
            except Exception as e:
                print(f"   âš ï¸ è¯äº‘ç”Ÿæˆå¤±è´¥: {e}")
            
            plt.close()
        
        # 6. èšç±»ä¸»é¢˜åˆ†å¸ƒ
        if hasattr(self, 'cluster_topics') and self.cluster_topics:
            print("ðŸŽ¯ ç”Ÿæˆèšç±»åˆ†æžå›¾...")
            fig6 = plt.figure(figsize=(14, 7))
            
            # æå–æ•°æ®
            topics = []
            sizes = []
            for cluster_id, info in self.cluster_topics.items():
                topics.append(info['name'])
                sizes.append(info['size'])
            
            # åˆ›å»ºæ°´å¹³æŸ±çŠ¶å›¾
            ax6 = plt.subplot(111)
            y_pos = np.arange(len(topics))
            colors_grad = plt.cm.Set3(np.linspace(0, 1, len(topics)))
            
            bars = ax6.barh(y_pos, sizes, color=colors_grad, alpha=0.8, edgecolor='navy', linewidth=2)
            
            # æ·»åŠ æ•°å€¼å’Œç™¾åˆ†æ¯”
            total_msgs = sum(sizes)
            for i, (bar, size) in enumerate(zip(bars, sizes)):
                percentage = size / total_msgs * 100
                ax6.text(size, i, f' {size}æ¡ ({percentage:.1f}%)', 
                        va='center', fontsize=11, fontweight='bold')
            
            ax6.set_yticks(y_pos)
            ax6.set_yticklabels(topics, fontsize=12, fontweight='bold')
            ax6.set_xlabel('æ¶ˆæ¯æ•°é‡', fontsize=13, fontweight='bold')
            ax6.set_title('ðŸŽ¯ èŠå¤©ä¸»é¢˜åˆ†å¸ƒåˆ†æž', fontsize=16, fontweight='bold', pad=20)
            ax6.grid(axis='x', alpha=0.3, linestyle='--')
            ax6.spines['top'].set_visible(False)
            ax6.spines['right'].set_visible(False)
            
            plt.tight_layout()
            plt.savefig('analysis_results/06_topic_clustering.png', dpi=200, bbox_inches='tight')
            print("   âœ… ä¿å­˜: 06_topic_clustering.png")
            plt.close()
        
        print(f"\nðŸ“ æ‰€æœ‰å›¾è¡¨å·²ä¿å­˜åˆ°: analysis_results/")
        print(f"   å…±ç”Ÿæˆ {'6' if hasattr(self, 'cluster_topics') else '5'} å¼ ç²¾ç¾Žå›¾è¡¨")

    
    def generate_ai_report(self):
        """ä½¿ç”¨DeepSeekç”ŸæˆAIåˆ†æžæŠ¥å‘Š"""
        if not self.api_key or self.api_key == "ä½ çš„APIå¯†é’¥":
            print("\nâš ï¸ æœªé…ç½®DeepSeek APIå¯†é’¥ï¼Œè·³è¿‡AIæŠ¥å‘Šç”Ÿæˆ")
            return None
        
        print("\n" + "="*60)
        print("ðŸ¤– ç”ŸæˆAIæ·±åº¦åˆ†æžæŠ¥å‘Š")
        print("="*60)
        
        # å‡†å¤‡åˆ†æžæ•°æ®
        sample_messages = self.df.sample(min(50, len(self.df)))['content'].tolist()
        sample_text = '\n'.join([f"- {msg}" for msg in sample_messages[:20]])
        
        prompt = f"""
        ä½œä¸ºä¸“ä¸šçš„ç¤¾äº¤å…³ç³»åˆ†æžå¸ˆï¼Œè¯·åˆ†æžä»¥ä¸‹å¾®ä¿¡èŠå¤©è®°å½•ï¼Œç»™å‡ºä¸“ä¸šè§è§£ï¼š
        
        ã€ç»Ÿè®¡æ•°æ®ã€‘
        - æ¶ˆæ¯æ€»æ•°: {self.stats.get('total_messages', 0)}
        - æ—¶é—´è·¨åº¦: {self.stats.get('duration_days', 0)}å¤©
        - æƒ…æ„Ÿåˆ†å¸ƒ: ç§¯æž{self.sentiment_results.get('positive', 0)}æ¡, ä¸­æ€§{self.sentiment_results.get('neutral', 0)}æ¡, æ¶ˆæž{self.sentiment_results.get('negative', 0)}æ¡
        
        ã€æ¶ˆæ¯æ ·æœ¬ã€‘
        {sample_text}
        
        è¯·ä»Žä»¥ä¸‹è§’åº¦åˆ†æžï¼š
        1. æ²Ÿé€šæ¨¡å¼ç‰¹ç‚¹
        2. æƒ…æ„Ÿè¡¨è¾¾æ–¹å¼
        3. å…³ç³»è´¨é‡è¯„ä¼°
        4. æ”¹å–„å»ºè®®
        
        è¯·ç”¨ç®€æ´ä¸“ä¸šçš„è¯­è¨€ï¼Œçªå‡ºå…³é”®å‘çŽ°ã€‚
        """
        
        try:
            print("ðŸ”„ æ­£åœ¨è°ƒç”¨DeepSeek API...")
            
            headers = {
                "Authorization": f"Bearer {self.api_key}",
                "Content-Type": "application/json"
            }
            
            payload = {
                "model": "deepseek-chat",
                "messages": [
                    {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„ç¤¾äº¤å…³ç³»åˆ†æžä¸“å®¶ï¼Œæ“…é•¿ä»ŽèŠå¤©è®°å½•ä¸­æ´žå¯Ÿäººé™…å…³ç³»æ¨¡å¼ã€‚"},
                    {"role": "user", "content": prompt}
                ],
                "max_tokens": 1500,
                "temperature": 0.7
            }
            
            response = requests.post(
                "https://api.deepseek.com/v1/chat/completions",
                headers=headers,
                json=payload,
                timeout=60
            )
            
            if response.status_code == 200:
                result = response.json()
                ai_analysis = result['choices'][0]['message']['content']
                print("âœ… AIåˆ†æžå®Œæˆ\n")
                print(ai_analysis)
                
                # ä¿å­˜æŠ¥å‘Š
                with open('analysis_results/ai_report.txt', 'w', encoding='utf-8') as f:
                    f.write(ai_analysis)
                print(f"\nðŸ“„ æŠ¥å‘Šå·²ä¿å­˜: analysis_results/ai_report.txt")
                
                return ai_analysis
            else:
                print(f"âŒ APIè°ƒç”¨å¤±è´¥: {response.status_code}")
                return None
                
        except Exception as e:
            print(f"âŒ AIåˆ†æžå‡ºé”™: {e}")
            return None
    
    def chat_simulator(self):
        """ç»ˆç«¯æ¨¡æ‹ŸèŠå¤©äº¤äº’"""
        print("\n" + "="*60)
        print("ðŸ’¬ èŠå¤©æ¨¡æ‹Ÿå™¨ï¼ˆè¾“å…¥'é€€å‡º'ç»“æŸï¼‰")
        print("="*60)
        
        if not self.api_key or self.api_key == "ä½ çš„APIå¯†é’¥":
            print("âš ï¸ æœªé…ç½®APIå¯†é’¥ï¼Œä½¿ç”¨è§„åˆ™å›žå¤æ¨¡å¼")
            use_api = False
        else:
            use_api = True
        
        # å­¦ä¹ èŠå¤©é£Žæ ¼
        print("\nðŸ“š æ­£åœ¨å­¦ä¹ èŠå¤©é£Žæ ¼...")
        chat_samples = self.df['content'].sample(min(100, len(self.df))).tolist()
        
        print("âœ… å‡†å¤‡å°±ç»ªï¼å¼€å§‹å¯¹è¯ï¼š\n")
        
        while True:
            user_input = input("ä½ : ").strip()
            
            if user_input in ['é€€å‡º', 'quit', 'exit', 'q']:
                print("ðŸ‘‹ å†è§ï¼")
                break
            
            if not user_input:
                continue
            
            # ç”Ÿæˆå›žå¤
            if use_api:
                reply = self._get_api_reply(user_input, chat_samples[:10])
            else:
                reply = self._get_rule_reply(user_input)
            
            print(f"AI: {reply}\n")
    
    def _get_api_reply(self, message, samples):
        """ä½¿ç”¨APIç”Ÿæˆå›žå¤"""
        try:
            headers = {
                "Authorization": f"Bearer {self.api_key}",
                "Content-Type": "application/json"
            }
            
            sample_text = '\n'.join(samples[:5])
            
            payload = {
                "model": "deepseek-chat",
                "messages": [
                    {"role": "system", "content": f"è¯·æ¨¡ä»¿ä»¥ä¸‹èŠå¤©é£Žæ ¼å›žå¤:\n{sample_text}"},
                    {"role": "user", "content": message}
                ],
                "max_tokens": 100,
                "temperature": 0.8
            }
            
            response = requests.post(
                "https://api.deepseek.com/v1/chat/completions",
                headers=headers,
                json=payload,
                timeout=30
            )
            
            if response.status_code == 200:
                return response.json()['choices'][0]['message']['content']
            else:
                return self._get_rule_reply(message)
                
        except:
            return self._get_rule_reply(message)
    
    def _get_rule_reply(self, message):
        """åŸºäºŽè§„åˆ™çš„å›žå¤"""
        message = message.lower()
        
        if any(word in message for word in ['ä½ å¥½', 'hi', 'hello', 'åœ¨å—']):
            return "åœ¨çš„ï¼æœ‰ä»€ä¹ˆå¯ä»¥å¸®ä½ çš„å—ï¼Ÿ"
        elif any(word in message for word in ['è°¢è°¢', 'æ„Ÿè°¢', 'thanks']):
            return "ä¸å®¢æ°”ï¼å¾ˆé«˜å…´èƒ½å¸®åˆ°ä½ ðŸ˜Š"
        elif '?' in message or 'å—' in message or 'å‘¢' in message:
            return "è¿™ä¸ªé—®é¢˜å¾ˆæœ‰æ„æ€ï¼Œè®©æˆ‘æƒ³æƒ³..."
        elif any(word in message for word in ['å“ˆå“ˆ', 'ðŸ˜„', 'ðŸ˜Š']):
            return "å“ˆå“ˆï¼Œä½ ä¹Ÿå¾ˆæœ‰è¶£ï¼"
        else:
            # ä»ŽåŽ†å²æ¶ˆæ¯ä¸­éšæœºé€‰æ‹©ä¸€æ¡
            if len(self.df) > 0:
                sample = self.df['content'].sample(1).iloc[0]
                if len(str(sample)) < 50:
                    return str(sample)
            return "å—¯å—¯ï¼Œæˆ‘ç†è§£ä½ çš„æ„æ€"
    
    def run_full_analysis(self, data_file):
        """è¿è¡Œå®Œæ•´åˆ†æžæµç¨‹"""
        print("="*60)
        print("ðŸš€ å¾®ä¿¡èŠå¤©è®°å½•ç»¼åˆåˆ†æžç³»ç»Ÿ")
        print("="*60)
        
        # 1. åŠ è½½æ•°æ®
        if data_file.endswith('.json'):
            if not self.load_json_data(data_file):
                return False
        else:
            print("âŒ æš‚ä¸æ”¯æŒè¯¥æ ¼å¼ï¼Œè¯·ä½¿ç”¨JSONæ–‡ä»¶")
            return False
        
        # 2. åŸºç¡€åˆ†æž
        self.basic_analysis()
        
        # 3. æƒ…æ„Ÿåˆ†æž
        self.simple_sentiment_analysis()
        
        # 4. è¯é¢‘åˆ†æž
        self.word_frequency_analysis()
        
        # 5. èšç±»åˆ†æž
        self.topic_clustering()
        
        # 6. ç”Ÿæˆå›¾è¡¨
        self.create_visualizations()
        
        # 7. AIæŠ¥å‘Šï¼ˆå¯é€‰ï¼‰
        print("\n" + "="*60)
        choice = input("æ˜¯å¦ç”ŸæˆAIæ·±åº¦æŠ¥å‘Šï¼Ÿ(y/nï¼Œéœ€è¦APIå¯†é’¥): ").strip().lower()
        if choice == 'y':
            self.generate_ai_report()
        
        # 8. äº¤äº’æ¨¡å¼ï¼ˆå¯é€‰ï¼‰
        print("\n" + "="*60)
        choice = input("æ˜¯å¦è¿›å…¥èŠå¤©æ¨¡æ‹Ÿå™¨ï¼Ÿ(y/n): ").strip().lower()
        if choice == 'y':
            self.chat_simulator()
        
        print("\n" + "="*60)
        print("âœ… åˆ†æžå®Œæˆï¼ç»“æžœå·²ä¿å­˜åˆ° analysis_results/ ç›®å½•")
        print("="*60)
        print("\nðŸ“Š ç”Ÿæˆçš„æ–‡ä»¶:")
        print("   ðŸ–¼ï¸ 01_daily_trend.png - æ¯æ—¥è¶‹åŠ¿å›¾")
        print("   ðŸ–¼ï¸ 02_sender_distribution.png - å‘é€è€…åˆ†å¸ƒ")
        print("   ðŸ–¼ï¸ 03_hourly_activity.png - æ´»è·ƒåº¦åˆ†æž")
        print("   ðŸ–¼ï¸ 04_sentiment_analysis.png - æƒ…æ„Ÿåˆ†æž")
        print("   ðŸ–¼ï¸ 05_wordcloud.png - è¯äº‘å›¾")
        print("   ðŸ–¼ï¸ 06_topic_clustering.png - ä¸»é¢˜èšç±»")
        if os.path.exists('analysis_results/ai_report.txt'):
            print("   ðŸ“„ ai_report.txt - AIåˆ†æžæŠ¥å‘Š")
        print("="*60)
        
        return True


def main():
    """ä¸»å‡½æ•°"""
    # è¯»å–é…ç½®
    try:
        from config import DEEPSEEK_API_KEY
        api_key = DEEPSEEK_API_KEY
    except:
        api_key = None
    
    # æŸ¥æ‰¾æ•°æ®æ–‡ä»¶
    data_files = [f for f in os.listdir('.') if f.endswith('.json')]
    
    if not data_files:
        print("âŒ æœªæ‰¾åˆ°JSONæ•°æ®æ–‡ä»¶")
        return
    
    print("ðŸ“‚ æ‰¾åˆ°ä»¥ä¸‹æ•°æ®æ–‡ä»¶:")
    for i, f in enumerate(data_files, 1):
        print(f"   {i}. {f}")
    
    if len(data_files) == 1:
        selected_file = data_files[0]
        print(f"\nè‡ªåŠ¨é€‰æ‹©: {selected_file}")
    else:
        try:
            idx = int(input("\nè¯·é€‰æ‹©æ–‡ä»¶åºå·: ")) - 1
            selected_file = data_files[idx]
        except:
            print("âŒ é€‰æ‹©æ— æ•ˆ")
            return
    
    # åˆ›å»ºåˆ†æžå™¨
    analyzer = WeChatAnalyzer(deepseek_api_key=api_key)
    
    # è¿è¡Œåˆ†æž
    analyzer.run_full_analysis(selected_file)


if __name__ == "__main__":
    main()
